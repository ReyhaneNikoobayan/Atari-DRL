# Atari-DRL

## Overview

This repository contains **my implementations** of **Deep Reinforcement Learning (DRL)** algorithms for training agents on **Atari 2600 games**.  
I implemented the full training pipeline, including environment setup, agent learning, evaluation, and logging.

This project is intended for:
- Studying and experimenting with DRL algorithms
- Training agents on classic Atari benchmarks
- Reproducing and extending reinforcement learning research

The codebase is structured to allow easy integration of new algorithms, environments, and experimental settings.

# ðŸŽ® Visual Advantage Actorâ€“Critic (A2C) for Atari Pong

This project implements a **from-scratch Advantage Actorâ€“Critic (A2C)** reinforcement learning algorithm trained on **Atari Pong (ALE/Pong-v5)** using **raw visual observations**.

The agent learns directly from pixel inputs through convolutional neural networks and is optimized using policy-gradient methods. All components â€” environment vectorization, preprocessing, optimization, evaluation, and video recording â€” are implemented manually using **PyTorch** and **Gymnasium**, without relying on high-level RL training frameworks.

---

## ðŸŽ¯ Project Motivation

This project was developed to gain a deep, practical understanding of:

- Actorâ€“critic reinforcement learning algorithms
- Policy-gradient optimization with value-function baselines
- End-to-end visual learning using convolutional networks
- Parallel environment training for improved sample efficiency

Atari Pong serves as a controlled benchmark for studying learning dynamics in visual reinforcement learning.

---

## ðŸ§  Algorithm: Advantage Actorâ€“Critic (A2C)

Advantage Actorâ€“Critic (A2C) is a **synchronous actorâ€“critic algorithm** that learns a policy and a value function simultaneously.

The algorithm consists of two models:

- **Actor**: learns a stochastic policy Ï€(a | s)
- **Critic**: estimates the state value V(s)

Training is performed using experiences collected from **multiple parallel environments**, and updates are applied synchronously for stability.

---

### Advantage Estimation

To reduce variance in policy-gradient updates, the actor is trained using an **advantage signal**.

Advantage at time t:

A(t) = Return(t) âˆ’ Value(s_t)

# The return is computed using bootstrapping:

Return(t) = Reward(t) + Î³ Ã— Return(t+1)


Where:
- Î³ is the discount factor
- Value(s_t) is the criticâ€™s estimate of the current state

---

### Optimization Objectives

**Actor Objective**

The actor is optimized to increase the probability of actions that lead to higher-than-expected returns:

Actor Loss =
âˆ’ log Ï€(a_t | s_t) Ã— Advantage(t)
âˆ’ Î² Ã— Entropy(Ï€)


- The entropy term encourages exploration
- Î² controls the strength of entropy regularization

---

**Critic Objective**

The critic is trained to regress the predicted value toward the observed return:

Critic Loss =
0.5 Ã— (Return(t) âˆ’ Value(s_t))Â² 

